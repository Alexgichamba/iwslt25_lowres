{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep for multilingual tokenizer training\n",
    "bigc_train_json_url = \"https://raw.githubusercontent.com/csikasote/bigc/main/data/bem/splits/train.jsonl\"\n",
    "bemba_speech_url = \"https://raw.githubusercontent.com/csikasote/BembaSpeech/refs/heads/master/bem/train.tsv\"\n",
    "alffa_fon_url = \"https://raw.githubusercontent.com/besacier/ALFFA_PUBLIC/refs/heads/master/ASR/FONGBE/data/train/text\"\n",
    "ffr_url = \"https://raw.githubusercontent.com/bonaventuredossou/ffr-v1/refs/heads/master/FFR-Dataset/FFR Dataset v2/ffr_dataset_v2.txt\"\n",
    "# local paths\n",
    "ffstc_path = \"../../mymy/train.csv\"\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "bigc_train_json_path = os.path.join(temp_dir, \"bem_train.jsonl\")\n",
    "bemba_speech_path = os.path.join(temp_dir, \"bemba_speech.tsv\")\n",
    "alffa_fon_path = os.path.join(temp_dir, \"alffa_fon.txt\")\n",
    "ffr_path = os.path.join(temp_dir, \"ffr.txt\")\n",
    "\n",
    "if not os.path.exists(bigc_train_json_path):\n",
    "    wget.download(bigc_train_json_url, bigc_train_json_path)\n",
    "\n",
    "if not os.path.exists(bemba_speech_path):\n",
    "    wget.download(bemba_speech_url, bemba_speech_path)\n",
    "\n",
    "if not os.path.exists(alffa_fon_path):\n",
    "    wget.download(alffa_fon_url, alffa_fon_path)\n",
    "\n",
    "if not os.path.exists(ffr_path):\n",
    "    wget.download(ffr_url, ffr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fon_sentence = \"Àgɔ́! Ǹkɔ̀xɔ̀ wá, wɛ̀tɛ̀ ànù yì bó, ɖò lɛ̃̀, ɔ̃̀, ɛ̃́, ì, ú, ò, ɖɔ́, gbè, kpó, xù, ʋù, zã́. Sɛ́ wɛ́ ɖé ɖé, mí xó wɛ̀, é yà hùn dɔ̀ wɛ̃́. ʋɛ̀, ɖè, ɖɔ̀, mɛ̃̀, yì, gbɔ̀, sɔ̃̀, lɛ́ nɔ̀ ɖó lɛ́. Àɖó lɛ̀ wɛ̃̀ dɔ̀, ɔ̀kpà kpɛ́!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line with bem_transcription: Umulumendo umo nasenda umupila elo umunakwe namukoba mukulu. and en_translation: \n",
      "Skipping line with bem_transcription: Mumbali ya ici ici cimashini cipompa amenshi kuli  icikulwa. and en_translation: .\n",
      "Skipping line with bem_transcription: Kabili umulumendo naikata pa kubeya kwakwe ilyo alekopwa icikope. and en_translation: .\n",
      "Skipping line with bem_transcription: Elyo kabili naisala na amenso yakwe. and en_translation: .\n",
      "Skipping line with bem_transcription: Abantu baleteya ubwangalo ubwakushelemuka pa meenshi makasa munshita yakasuba. and en_translation: .\n",
      "Skipping line with bem_transcription: Umwana naikala elyo alelya. and en_translation: .\n",
      "Skipping line with bem_transcription: Imbwa na ingombe  fili mwibala. and en_translation: .\n",
      "Skipping line with bem_transcription: Shitaata ali mumuputule wakulilamo naikala petebulo. and en_translation: .\n",
      "Skipping line with bem_transcription: Uyu umulumendo wa caice alepeluka ku ntambo iikakilwe ku mutanto uwakashika. and en_translation: .\n",
      "Skipping line with bem_transcription: Uyu umwaume afwele akaputula aka katapakatapa elyo na ishati lya makumbi makumbi . and en_translation: .\n",
      "Skipping line with bem_transcription: Cipalile kwati aba abaume balesefya uyu umwaume uwikele pacipuna icafiita. and en_translation: .\n",
      "Skipping line with bem_transcription: Elyo kabili uyu mwaume afwele ishati lyakatapakatapa elyo ne toloshi lyafiita. and en_translation: .\n",
      "Skipping line with bem_transcription: Balefwaya ukuitomona pa milomo. and en_translation: .\n",
      "Skipping line with bem_transcription: Kunuma yabo kuli amacinga ayengi yantu abanabo babikile. and en_translation: .\n",
      "Skipping line with bem_transcription: uyu umulumendo wacaice afwele akaputula akafita umupile imilaini iyakashika elya na ishati ilya makumbi makumbi. and en_translation: .\n",
      "Skipping line with bem_transcription: Bushe uyu umwaume ufwele amakalashi iyafiita alecita finshi? and en_translation: .\n",
      "Skipping line with bem_transcription: Aba bantu bali mucikulwa umo bateyala amangalo ayalekanalekana. and en_translation: .\n",
      "Loaded 82358 lines in bigc train data\n",
      "Loaded 29545 lines in ffstc data\n",
      "Loaded 12421 lines in bemba_speech data\n",
      "Loaded 8234 lines in alffa_fon data\n",
      "Loaded 54748 lines in ffr data\n",
      "\n",
      "Total bem lines: 94779\n",
      "Total en lines: 82358\n",
      "Total fr lines: 84293\n",
      "Total fon lines: 62983\n"
     ]
    }
   ],
   "source": [
    "bem_lines = []\n",
    "en_lines = []\n",
    "fra_lines = []\n",
    "fon_lines = []\n",
    "# load data\n",
    "with open(bigc_train_json_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    # extract bem_transcription and en_translation\n",
    "\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        bem_transcription = line[\"bem_transcription\"].strip()\n",
    "        en_translation = line[\"en_translation\"].strip()\n",
    "\n",
    "        if bem_transcription != \".\" and en_translation != \".\" and bem_transcription != \"\" and en_translation != \"\":\n",
    "            bem_lines.append(bem_transcription)\n",
    "            en_lines.append(en_translation)\n",
    "        else :\n",
    "            print(f\"Skipping line with bem_transcription: {bem_transcription} and en_translation: {en_translation}\")\n",
    "    print(f\"Loaded {len(bem_lines)} lines in bigc train data\")\n",
    "big_c_num = len(bem_lines)\n",
    "\n",
    "# open ffstc\n",
    "df = pd.read_csv(ffstc_path)\n",
    "for i, row in df.iterrows():\n",
    "    fr_translation = row[\"utterance\"].strip()\n",
    "\n",
    "    if fr_translation != \".\" and fr_translation != \"\":\n",
    "        fra_lines.append(fr_translation)\n",
    "print(f\"Loaded {len(fra_lines)} lines in ffstc data\")\n",
    "ffstc_num = len(fra_lines)\n",
    "\n",
    "# open bemba_speech\n",
    "with open(bemba_speech_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bemba_speech_lines = f.readlines()\n",
    "    for line in bemba_speech_lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            _, bem = line.split(\"\\t\")\n",
    "            bem = bem.strip()\n",
    "            # write\n",
    "            if bem != \".\" and bem != \"\":\n",
    "                bem_lines.append(bem)\n",
    "print(f\"Loaded {len(bem_lines)-big_c_num} lines in bemba_speech data\")\n",
    "\n",
    "\n",
    "# open alffa_fon\n",
    "with open(alffa_fon_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    alffa_fon_lines = f.readlines()\n",
    "    for line in alffa_fon_lines:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            _, fon = line.split(' ', 1)  # Split on first space only\n",
    "            fon = fon.strip()\n",
    "            # write\n",
    "            if fon != \".\" and fon != \"\":\n",
    "                fon_lines.append(fon)\n",
    "print(f\"Loaded {len(fon_lines)} lines in alffa_fon data\")\n",
    "alffa_fon_num = len(fon_lines)\n",
    "\n",
    "# open ffr\n",
    "with open(ffr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ffr_lines = f.readlines()\n",
    "    for line in ffr_lines:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                fon, fra = parts\n",
    "                fon = fon.strip()\n",
    "                fra = fra.strip()\n",
    "                # write\n",
    "                if fon != \".\" and fon != \"\":\n",
    "                    fon_lines.append(fon)\n",
    "                if fra != \".\" and fra != \"\":\n",
    "                    fra_lines.append(fra)\n",
    "print(f\"Loaded {len(fra_lines)-ffstc_num} lines in ffr data\")\n",
    "\n",
    "# add fon_sentence to fon_lines\n",
    "fon_lines.append(fon_sentence)\n",
    "# summary stats\n",
    "print()\n",
    "print(f\"Total bem lines: {len(bem_lines)}\")\n",
    "print(f\"Total en lines: {len(en_lines)}\")\n",
    "print(f\"Total fr lines: {len(fra_lines)}\")\n",
    "print(f\"Total fon lines: {len(fon_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump all data to temp file in temp dir\n",
    "all_data_path = os.path.join(temp_dir, \"all_data.txt\")\n",
    "with open(all_data_path, \"w\") as f:\n",
    "    for bem, en, fr, fon in zip(bem_lines, en_lines, fra_lines, fon_lines):\n",
    "        f.write(f\"{bem}\\n\")\n",
    "        f.write(f\"{en}\\n\")\n",
    "        f.write(f\"{fr}\\n\")\n",
    "        f.write(f\"{fon}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251951\n"
     ]
    }
   ],
   "source": [
    "!cat temp/all_data.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete temp files\n",
    "os.remove(bigc_train_json_path)\n",
    "os.remove(all_data_path)\n",
    "# remove temp dir\n",
    "os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
