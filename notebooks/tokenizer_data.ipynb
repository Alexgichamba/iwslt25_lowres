{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep for multilingual tokenizer training\n",
    "bigc_train_json_url = \"https://raw.githubusercontent.com/csikasote/bigc/main/data/bem/splits/train.jsonl\"\n",
    "ffstc_path = \"/ocean/projects/cis210027p/gichamba/iwslt25/mymy/train.csv\"\n",
    "bem_en_path = \"/ocean/projects/cis210027p/gichamba/iwslt25/bem_en.txt\"\n",
    "fon_fr_path = \"/ocean/projects/cis210027p/gichamba/iwslt25/fon_fr.txt\"\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "bigc_train_json_path = os.path.join(temp_dir, \"train.jsonl\")\n",
    "\n",
    "if not os.path.exists(bigc_train_json_path):\n",
    "    wget.download(bigc_train_json_url, bigc_train_json_path)\n",
    "\n",
    "bem_lines = []\n",
    "en_lines = []\n",
    "fr_lines = []\n",
    "fon_lines = []\n",
    "# load data\n",
    "with open(bigc_train_json_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    # extract bem_transcription and en_translation\n",
    "\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        bem_transcription = line[\"bem_transcription\"].strip()\n",
    "        en_translation = line[\"en_translation\"].strip()\n",
    "\n",
    "        if bem_transcription != \".\" and en_translation != \".\" and bem_transcription != \"\" and en_translation != \"\":\n",
    "            # ensure sentence capitalization\n",
    "            bem_transcription[0].upper() + bem_transcription[1:]\n",
    "            en_translation[0].upper() + en_translation[1:]\n",
    "            bem_lines.append(bem_transcription)\n",
    "            en_lines.append(en_translation)\n",
    "        else :\n",
    "            print(f\"Skipping line with bem_transcription: {bem_transcription} and en_translation: {en_translation}\")\n",
    "    print(f\"Loaded {len(bem_lines)} lines in bigc train data\")\n",
    "big_c_num = len(bem_lines)\n",
    "\n",
    "# open ffstc\n",
    "df = pd.read_csv(ffstc_path)\n",
    "for i, row in df.iterrows():\n",
    "    fr_translation = row[\"utterance\"].strip()\n",
    "\n",
    "    if fr_translation != \".\" and fr_translation != \"\":\n",
    "        # ensure sentence capitalization\n",
    "        fr_translation[0].upper() + fr_translation[1:]\n",
    "        fr_lines.append(fr_translation)\n",
    "print(f\"Loaded {len(fr_lines)} lines in ffstc data\")\n",
    "\n",
    "# open bem_en\n",
    "with open(bem_en_path, \"r\") as f:\n",
    "    bem_en_lines = f.readlines()\n",
    "    for line in bem_en_lines:\n",
    "        line = line.strip()\n",
    "        bem, en = line.split(\" || \")\n",
    "        bem_lines.append(bem)\n",
    "        en_lines.append(en)\n",
    "    print(f\"Loaded {len(bem_lines) - big_c_num} lines in bem_en data\")\n",
    "\n",
    "# open fon_fr\n",
    "with open(fon_fr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    fon_fr_lines = f.readlines()\n",
    "    for line in fon_fr_lines:\n",
    "        line = line.strip()\n",
    "        fon, fr = line.split(\" || \")\n",
    "        fon_lines.append(fon)\n",
    "        fr_lines.append(fr)\n",
    "    print(f\"Loaded {len(fon_lines)} lines in fon_fr data\")\n",
    "\n",
    "# summary stats\n",
    "print(f\"Total bem lines: {len(bem_lines)}\")\n",
    "print(f\"Total en lines: {len(en_lines)}\")\n",
    "print(f\"Total fr lines: {len(fr_lines)}\")\n",
    "print(f\"Total fon lines: {len(fon_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump all data to temp file in temp dir\n",
    "all_data_path = os.path.join(temp_dir, \"all_data.txt\")\n",
    "with open(all_data_path, \"w\") as f:\n",
    "    for bem, en, fr, fon in zip(bem_lines, en_lines, fr_lines, fon_lines):\n",
    "        f.write(f\"{bem}\\n\")\n",
    "        f.write(f\"{en}\\n\")\n",
    "        f.write(f\"{fr}\\n\")\n",
    "        f.write(f\"{fon}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat temp/all_data.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete temp files\n",
    "os.remove(bigc_train_json_path)\n",
    "os.remove(all_data_path)\n",
    "# remove temp dir\n",
    "os.rmdir(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
