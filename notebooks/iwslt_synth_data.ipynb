{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fg-bJKXa3tW",
        "outputId": "6fef570f-2284-40c4-e6f5-c1f670f4da2d"
      },
      "outputs": [],
      "source": [
        "!pip install -q wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhsCualH9JC9",
        "outputId": "8e305c3f-c363-490d-a6a8-9911ecf102ea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import wget\n",
        "import zipfile\n",
        "import regex as re\n",
        "\n",
        "# Define URLs\n",
        "ted_moses_url = \"https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-fr.txt.zip\"\n",
        "europarl_moses_url = \"https://object.pouta.csc.fi/OPUS-Europarl/v8/moses/en-fr.txt.zip\"\n",
        "\n",
        "download_dir = \".\"\n",
        "\n",
        "# File paths for downloads\n",
        "ted_zip = os.path.join(download_dir, \"ted.zip\")\n",
        "europarl_zip = os.path.join(download_dir, \"europarl.zip\")\n",
        "\n",
        "\n",
        "if not os.path.exists(ted_zip):\n",
        "    print(\"Downloading TED parallel data...\")\n",
        "    ted_path = wget.download(ted_moses_url, out=ted_zip)\n",
        "    print(\"\\nTED download complete.\")\n",
        "else:\n",
        "    print(\"TED zip already exists, skipping download.\")\n",
        "    ted_path = ted_zip\n",
        "\n",
        "if not os.path.exists(europarl_zip):\n",
        "    print(\"\\nDownloading Europarl parallel data...\")\n",
        "    europarl_path = wget.download(europarl_moses_url, out=europarl_zip)\n",
        "    print(\"\\nEuroparl download complete.\")\n",
        "else:\n",
        "    print(\"Europarl zip already exists, skipping download.\")\n",
        "    europarl_path = europarl_zip\n",
        "\n",
        "# Define extraction directories\n",
        "ted_dir = os.path.join(download_dir, \"TED\")\n",
        "europarl_dir = os.path.join(download_dir, \"Europarl\")\n",
        "\n",
        "# Define extraction directories\n",
        "ted_dir = os.path.join(download_dir, \"TED\")\n",
        "europarl_dir = os.path.join(download_dir, \"Europarl\")\n",
        "\n",
        "# Create extraction directories if they don't exist\n",
        "os.makedirs(ted_dir, exist_ok=True)\n",
        "os.makedirs(europarl_dir, exist_ok=True)\n",
        "\n",
        "# Extract Ted data\n",
        "if not os.listdir(ted_dir):\n",
        "    print(\"\\nExtracting TED parallel data...\")\n",
        "    with zipfile.ZipFile(ted_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(ted_dir)\n",
        "    print(\"TED extraction complete.\")\n",
        "else:\n",
        "    print(\"TED directory already contains files, skipping extraction.\")\n",
        "\n",
        "# Extract Europarl data\n",
        "if not os.listdir(europarl_dir):\n",
        "    print(\"\\nExtracting Europarl parallel data...\")\n",
        "    with zipfile.ZipFile(europarl_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(europarl_dir)\n",
        "    print(\"Europarl extraction complete.\")\n",
        "else:\n",
        "    print(\"Europarl directory already contains files, skipping extraction.\")\n",
        "\n",
        "\n",
        "#  Define file paths\n",
        "\n",
        "ted_fr = \"TED2020.en-fr.fr\"\n",
        "ted_en = \"TED2020.en-fr.en\"\n",
        "europarl_fr = \"Europarl.en-fr.fr\"\n",
        "europarl_en = \"Europarl.en-fr.en\"\n",
        "\n",
        "# Full paths to\n",
        "ted_fr_path = os.path.join(ted_dir, ted_fr)\n",
        "ted_en_path = os.path.join(ted_dir, ted_en)\n",
        "europarl_fr_path = os.path.join(europarl_dir, europarl_fr)\n",
        "europarl_en_path = os.path.join(europarl_dir, europarl_en)\n",
        "\n",
        "\n",
        "# ensure lines match for all\n",
        "def count_lines(file_path):\n",
        "    \"\"\"\n",
        "    Count the number of lines in a file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        int: Number of lines in the file\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "def verify_parallel_corpus(files):\n",
        "    \"\"\"\n",
        "    Verify that all files have the same number of lines.\n",
        "\n",
        "    Args:\n",
        "        files (list): List of file paths to compare\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If files have different line counts\n",
        "    \"\"\"\n",
        "    line_counts = {}\n",
        "\n",
        "    # Count lines for each file\n",
        "    for file_path in files:\n",
        "        line_counts[file_path] = count_lines(file_path)\n",
        "\n",
        "    # Get the first line count to compare against\n",
        "    first_file = list(line_counts.keys())[0]\n",
        "    first_count = line_counts[first_file]\n",
        "\n",
        "    # Verify all files have the same line count\n",
        "    mismatched_files = []\n",
        "    for file_path, count in line_counts.items():\n",
        "        if count != first_count:\n",
        "            mismatched_files.append((file_path, count))\n",
        "\n",
        "    # Raise an error if any mismatches found\n",
        "    if mismatched_files:\n",
        "        error_message = \"Line count mismatch in parallel corpus:\\n\"\n",
        "        for file_path, count in mismatched_files:\n",
        "            error_message += f\"{file_path}: {count} lines (expected {first_count})\\n\"\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    print(\"All files have matching line counts:\")\n",
        "    for file_path, count in line_counts.items():\n",
        "        print(f\"{os.path.basename(file_path)}: {count} lines\")\n",
        "\n",
        "# Check that all files have matching lines\n",
        "verify_parallel_corpus([\n",
        "    ted_fr_path,\n",
        "    ted_en_path,\n",
        "])\n",
        "verify_parallel_corpus([\n",
        "    europarl_en_path, europarl_fr_path])\n",
        "\n",
        "def print_parallel_samples(en_path, fr_path, num_samples=5):\n",
        "    \"\"\"\n",
        "    Print parallel sentence samples from English and French files.\n",
        "\n",
        "    Args:\n",
        "        en_path (str): Path to English file\n",
        "        fr_path (str): Path to French file\n",
        "        num_samples (int): Number of samples to print\n",
        "    \"\"\"\n",
        "    with open(en_path, 'r', encoding='utf-8') as en_file, \\\n",
        "         open(fr_path, 'r', encoding='utf-8') as fr_file:\n",
        "\n",
        "        print(f\"Parallel Corpus Samples ({num_samples} lines):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            en_line = en_file.readline().strip()\n",
        "            fr_line = fr_file.readline().strip()\n",
        "\n",
        "            print(f\"EN [{i+1}]: {en_line}\")\n",
        "            print(f\"FR [{i+1}]: {fr_line}\")\n",
        "            print()\n",
        "\n",
        "# Print samples for TED corpus\n",
        "print(\"TED Corpus Samples:\")\n",
        "print_parallel_samples(ted_en_path, ted_fr_path, 20)\n",
        "\n",
        "# Print samples for Europarl corpus\n",
        "print(\"\\nEuroparl Corpus Samples:\")\n",
        "print_parallel_samples(europarl_en_path, europarl_fr_path, 20)\n",
        "\n",
        "# def is_valid(line):\n",
        "#     \"\"\"\n",
        "#     Determines if a line is likely valid based on specific patterns.\n",
        "#     Returns True if the line contains Fongbe, False otherwise.\n",
        "#     \"\"\"\n",
        "#     # Skip empty lines\n",
        "#     if not line.strip():\n",
        "#         return False\n",
        "\n",
        "#     # Check for non-Fongbe scripts\n",
        "#     if re.search(r'[\\u0400-\\u04FF]', line):  # Cyrillic\n",
        "#         return False\n",
        "#     if re.search(r'[\\u4E00-\\u9FFF]', line):  # Chinese\n",
        "#         return False\n",
        "#     if re.search(r'[\\u0600-\\u06FF]', line):  # Arabic\n",
        "#         return False\n",
        "#     if re.search(r'[ÂÃÇÊÎÏÑÔÖ×ÜÞßåæçðñõ÷øýþÿİŞ]', line):\n",
        "#         return False\n",
        "#     # Default to not Fongbe if no clear indicators\n",
        "#     return True\n",
        "\n",
        "# dirty_lines_fon = []\n",
        "# dirty_lines_bem = []\n",
        "\n",
        "# # Print a random sample from each pair to verify content\n",
        "# def print_sample(file1_path, file2_path, lang1, lang2, sample_count=3):\n",
        "#     with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
        "#         lines1 = f1.readlines()\n",
        "#         lines2 = f2.readlines()\n",
        "\n",
        "#         import random\n",
        "#         indices = random.sample(range(min(len(lines1), len(lines2))), sample_count)\n",
        "\n",
        "#         print(f\"\\nSample of {sample_count} random {lang1}-{lang2} pairs:\")\n",
        "#         for idx in indices:\n",
        "#             print(f\"{lang1}: {lines1[idx].strip()}\")\n",
        "#             print(f\"{lang2}: {lines2[idx].strip()}\")\n",
        "#             print(\"-\" * 40)\n",
        "#         countr=0\n",
        "#         for i,line1,line2 in enumerate(lines1, lines2)\n",
        "#           valid = is_valid(line1)\n",
        "#           if valid is False:\n",
        "#             countr+=1\n",
        "#             dirty_lines.append(i)\n",
        "#         print(f\"found {countr} lines\")\n",
        "\n",
        "# print_sample(bem_path, en_path, \"Bemba\", \"English\")\n",
        "# print_sample(fon_path, fra_path, \"Fon\", \"French\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVX3JRRoI1Uo",
        "outputId": "31cec062-8b6e-4b4a-a5b7-3fddbfeb10dd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Set, Tuple, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MultilingualInverseNumberNormalizer:\n",
        "    \"\"\"\n",
        "    Base class for language-specific inverse number normalizers\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Mappings for converting digits/numbers to words\n",
        "        self.digits: Dict[int, str] = {}\n",
        "        self.teens: Dict[int, str] = {}\n",
        "        self.tens: Dict[int, str] = {}\n",
        "        self.multipliers: Dict[int, str] = {}\n",
        "\n",
        "        # Special cases\n",
        "        self.ordinal_suffixes: Dict[str, str] = {}\n",
        "        self.currency_symbols: Dict[str, str] = {}\n",
        "\n",
        "        # Patterns for detecting numbers, currencies, etc.\n",
        "        self.number_pattern = r'\\b\\d+\\b'\n",
        "        self.decimal_pattern = r'\\b\\d+\\.\\d+\\b'\n",
        "        self.ordinal_pattern = r'\\b\\d+(st|nd|rd|th)\\b'\n",
        "        self.currency_pattern = r'(?:([$€£])(\\d+(?:[,.]\\d+)?)|(\\d+(?:[,.]\\d+)?)([€£$]))'\n",
        "        self.percentage_pattern = r'(\\d+(?:\\.\\d+)?)%'\n",
        "\n",
        "    def convert_number(self, num: int) -> str:\n",
        "        \"\"\"\n",
        "        Convert a number to its word representation\n",
        "        Must be implemented by subclasses\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def convert_decimal(self, num: float) -> str:\n",
        "        \"\"\"\n",
        "        Convert a decimal number to its word representation\n",
        "        Must be implemented by subclasses\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def convert_ordinal(self, match: re.Match) -> str:\n",
        "        \"\"\"\n",
        "        Convert an ordinal number (e.g. \"1st\") to its word representation\n",
        "        Must be implemented by subclasses\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def convert_currency(self, match: re.Match) -> str:\n",
        "        \"\"\"\n",
        "        Convert a currency amount to its word representation\n",
        "        Must be implemented by subclasses\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def convert_percentage(self, match: re.Match) -> str:\n",
        "        \"\"\"\n",
        "        Convert a percentage to its word representation\n",
        "        Must be implemented by subclasses\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def __call__(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Main method to perform inverse text normalization\n",
        "        \"\"\"\n",
        "        # Process ordinals first (to avoid conflict with basic numbers)\n",
        "        text = re.sub(self.ordinal_pattern, lambda m: self.convert_ordinal(m), text)\n",
        "\n",
        "        # Process currency\n",
        "        text = re.sub(self.currency_pattern, lambda m: self.convert_currency(m), text)\n",
        "\n",
        "        # Process percentages\n",
        "        text = re.sub(self.percentage_pattern, lambda m: self.convert_percentage(m), text)\n",
        "\n",
        "        # Process decimals - pass the whole string for handling comma/period\n",
        "        text = re.sub(self.decimal_pattern,\n",
        "                      lambda m: self.convert_decimal(m.group(0)), text)\n",
        "\n",
        "        # Process regular numbers\n",
        "        text = re.sub(self.number_pattern,\n",
        "                     lambda m: self.convert_number(int(m.group(0))), text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "class EnglishInverseNumberNormalizer(MultilingualInverseNumberNormalizer):\n",
        "    \"\"\"\n",
        "    Convert arabic numbers into spelled-out English text\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.digits = {\n",
        "            0: \"zero\",\n",
        "            1: \"one\",\n",
        "            2: \"two\",\n",
        "            3: \"three\",\n",
        "            4: \"four\",\n",
        "            5: \"five\",\n",
        "            6: \"six\",\n",
        "            7: \"seven\",\n",
        "            8: \"eight\",\n",
        "            9: \"nine\"\n",
        "        }\n",
        "\n",
        "        self.teens = {\n",
        "            10: \"ten\",\n",
        "            11: \"eleven\",\n",
        "            12: \"twelve\",\n",
        "            13: \"thirteen\",\n",
        "            14: \"fourteen\",\n",
        "            15: \"fifteen\",\n",
        "            16: \"sixteen\",\n",
        "            17: \"seventeen\",\n",
        "            18: \"eighteen\",\n",
        "            19: \"nineteen\"\n",
        "        }\n",
        "\n",
        "        self.tens = {\n",
        "            2: \"twenty\",\n",
        "            3: \"thirty\",\n",
        "            4: \"forty\",\n",
        "            5: \"fifty\",\n",
        "            6: \"sixty\",\n",
        "            7: \"seventy\",\n",
        "            8: \"eighty\",\n",
        "            9: \"ninety\"\n",
        "        }\n",
        "\n",
        "        self.multipliers = {\n",
        "            100: \"hundred\",\n",
        "            1_000: \"thousand\",\n",
        "            1_000_000: \"million\",\n",
        "            1_000_000_000: \"billion\",\n",
        "            1_000_000_000_000: \"trillion\"\n",
        "        }\n",
        "\n",
        "        # Sorted multipliers from largest to smallest for processing\n",
        "        self.sorted_multipliers = sorted(self.multipliers.keys(), reverse=True)\n",
        "\n",
        "        self.ordinal_suffixes = {\n",
        "            \"1\": \"st\",\n",
        "            \"2\": \"nd\",\n",
        "            \"3\": \"rd\"\n",
        "        }\n",
        "\n",
        "        self.ordinal_words = {\n",
        "            1: \"first\",\n",
        "            2: \"second\",\n",
        "            3: \"third\",\n",
        "            4: \"fourth\",\n",
        "            5: \"fifth\",\n",
        "            8: \"eighth\",\n",
        "            9: \"ninth\",\n",
        "            12: \"twelfth\"\n",
        "        }\n",
        "\n",
        "        self.currency_symbols = {\n",
        "            \"$\": \"dollar\",\n",
        "            \"£\": \"pound\",\n",
        "            \"€\": \"euro\",\n",
        "            \"¢\": \"cent\"\n",
        "        }\n",
        "\n",
        "        # Update patterns with English-specific regex\n",
        "        self.ordinal_pattern = r'\\b(\\d+)(st|nd|rd|th)\\b'\n",
        "        self.currency_pattern = r'(?:([$€£])(\\d+(?:[,.]\\d+)?)|(\\d+(?:[,.]\\d+)?)([€£$]))'\n",
        "        self.cents_pattern = r'¢(\\d+)'\n",
        "\n",
        "    def convert_number(self, num: int) -> str:\n",
        "        \"\"\"Convert a number to its word representation in English\"\"\"\n",
        "        if num == 0:\n",
        "            return \"zero\"\n",
        "\n",
        "        if num < 0:\n",
        "            return \"negative \" + self.convert_number(abs(num))\n",
        "\n",
        "        if num < 10:\n",
        "            return self.digits[num]\n",
        "\n",
        "        if num < 20:\n",
        "            return self.teens[num]\n",
        "\n",
        "        if num < 100:\n",
        "            tens_digit = num // 10\n",
        "            units_digit = num % 10\n",
        "\n",
        "            if units_digit == 0:\n",
        "                return self.tens[tens_digit]\n",
        "            else:\n",
        "                return f\"{self.tens[tens_digit]}-{self.digits[units_digit]}\"\n",
        "\n",
        "        # Handle larger numbers recursively\n",
        "        for multiplier in self.sorted_multipliers:\n",
        "            if num >= multiplier:\n",
        "                quotient, remainder = divmod(num, multiplier)\n",
        "\n",
        "                if remainder == 0:\n",
        "                    return f\"{self.convert_number(quotient)} {self.multipliers[multiplier]}\"\n",
        "                else:\n",
        "                    if multiplier == 100:\n",
        "                        conjunction = \" and \" if remainder < 100 else \" \"\n",
        "                    else:\n",
        "                        conjunction = \" \"\n",
        "\n",
        "                    return f\"{self.convert_number(quotient)} {self.multipliers[multiplier]}{conjunction}{self.convert_number(remainder)}\"\n",
        "\n",
        "        # Should never reach here given the checks above\n",
        "        return str(num)\n",
        "\n",
        "    def convert_decimal(self, num: Union[float, str]) -> str:\n",
        "        \"\"\"Convert a decimal number to its word representation in English\"\"\"\n",
        "        # Ensure num is a float\n",
        "        if isinstance(num, str):\n",
        "            # Replace comma with period for float conversion\n",
        "            num_str = num.replace(',', '.')\n",
        "            num = float(num_str)\n",
        "\n",
        "        # Handle scientific notation by converting to regular float string\n",
        "        num_str = str(num)\n",
        "\n",
        "        if 'e' in num_str.lower():\n",
        "            # For scientific notation, convert to regular notation\n",
        "            num_str = f\"{num:.10f}\".rstrip('0').rstrip('.')\n",
        "\n",
        "        # Split on the decimal point\n",
        "        parts = num_str.split('.')\n",
        "        integer_part = int(parts[0])\n",
        "\n",
        "        integer_words = self.convert_number(integer_part)\n",
        "\n",
        "        if len(parts) > 1:\n",
        "            # Handle the decimal part digit by digit\n",
        "            decimal_part = parts[1]\n",
        "            decimal_words = \" \".join(self.digits[int(digit)] for digit in decimal_part)\n",
        "            return f\"{integer_words} point {decimal_words}\"\n",
        "        else:\n",
        "            # No decimal part\n",
        "            return integer_words\n",
        "\n",
        "    def convert_ordinal(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert an ordinal number to its word representation in English\"\"\"\n",
        "        number = int(match.group(1))\n",
        "\n",
        "        # Special cases\n",
        "        if number in self.ordinal_words:\n",
        "            return self.ordinal_words[number]\n",
        "\n",
        "        # General cases\n",
        "        if number < 10:\n",
        "            # Single digit ordinals\n",
        "            return f\"{self.digits[number]}th\"\n",
        "        elif number < 20:\n",
        "            # Teens ordinals\n",
        "            base_word = self.teens[number]\n",
        "            return f\"{base_word[:-1]}th\" if base_word.endswith('e') else f\"{base_word}th\"\n",
        "        elif number < 100:\n",
        "            # Tens\n",
        "            tens_digit = number // 10\n",
        "            units_digit = number % 10\n",
        "\n",
        "            if units_digit == 0:\n",
        "                base_word = self.tens[tens_digit]\n",
        "                return f\"{base_word[:-1]}ieth\"\n",
        "\n",
        "            # Compound numbers\n",
        "            tens_word = self.tens[tens_digit]\n",
        "\n",
        "            if units_digit in self.ordinal_words:\n",
        "                units_word = self.ordinal_words[units_digit]\n",
        "            else:\n",
        "                units_word = f\"{self.digits[units_digit]}th\"\n",
        "\n",
        "            return f\"{tens_word}-{units_word}\"\n",
        "        else:\n",
        "            # For larger numbers, convert the number and then modify the last word\n",
        "            number_words = self.convert_number(number).split()\n",
        "            last_word = number_words[-1]\n",
        "\n",
        "            # Apply ordinal rules to the last word\n",
        "            if last_word == \"one\":\n",
        "                number_words[-1] = \"first\"\n",
        "            elif last_word == \"two\":\n",
        "                number_words[-1] = \"second\"\n",
        "            elif last_word == \"three\":\n",
        "                number_words[-1] = \"third\"\n",
        "            elif last_word == \"five\":\n",
        "                number_words[-1] = \"fifth\"\n",
        "            elif last_word == \"eight\":\n",
        "                number_words[-1] = \"eighth\"\n",
        "            elif last_word == \"nine\":\n",
        "                number_words[-1] = \"ninth\"\n",
        "            elif last_word == \"twelve\":\n",
        "                number_words[-1] = \"twelfth\"\n",
        "            elif last_word.endswith('y'):\n",
        "                number_words[-1] = last_word[:-1] + \"ieth\"\n",
        "            else:\n",
        "                number_words[-1] = last_word + \"th\"\n",
        "\n",
        "            return \" \".join(number_words)\n",
        "\n",
        "    def convert_currency(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert a currency amount to its word representation in English\"\"\"\n",
        "        # Handle both pattern formats (symbol before or after amount)\n",
        "        if match.group(1) is not None:\n",
        "            # Format: $42,50\n",
        "            symbol = match.group(1)\n",
        "            amount = match.group(2).replace(',', '.')\n",
        "        else:\n",
        "            # Format: 42,50€\n",
        "            symbol = match.group(4)\n",
        "            amount = match.group(3).replace(',', '.')\n",
        "\n",
        "        if '.' in amount:\n",
        "            dollars, cents = amount.split('.')\n",
        "            dollars_int = int(dollars)\n",
        "            cents_int = int(cents)\n",
        "\n",
        "            dollars_text = self.convert_number(dollars_int)\n",
        "\n",
        "            if cents_int == 0:\n",
        "                return f\"{dollars_text} {self.currency_symbols[symbol]}s\"\n",
        "\n",
        "            cents_text = self.convert_number(cents_int)\n",
        "\n",
        "            if dollars_int == 1:\n",
        "                return f\"one {self.currency_symbols[symbol]} and {cents_text} cents\"\n",
        "            else:\n",
        "                return f\"{dollars_text} {self.currency_symbols[symbol]}s and {cents_text} cents\"\n",
        "        else:\n",
        "            amount_int = int(amount)\n",
        "            amount_text = self.convert_number(amount_int)\n",
        "\n",
        "            if amount_int == 1:\n",
        "                return f\"one {self.currency_symbols[symbol]}\"\n",
        "            else:\n",
        "                return f\"{amount_text} {self.currency_symbols[symbol]}s\"\n",
        "\n",
        "    def convert_percentage(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert a percentage to its word representation in English\"\"\"\n",
        "        value = match.group(1)\n",
        "\n",
        "        if '.' in value:\n",
        "            return f\"{self.convert_decimal(float(value))} percent\"\n",
        "        else:\n",
        "            return f\"{self.convert_number(int(value))} percent\"\n",
        "\n",
        "\n",
        "class FrenchInverseNumberNormalizer(MultilingualInverseNumberNormalizer):\n",
        "    \"\"\"\n",
        "    Convert arabic numbers into spelled-out French text\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.digits = {\n",
        "            0: \"zéro\",\n",
        "            1: \"un\",\n",
        "            2: \"deux\",\n",
        "            3: \"trois\",\n",
        "            4: \"quatre\",\n",
        "            5: \"cinq\",\n",
        "            6: \"six\",\n",
        "            7: \"sept\",\n",
        "            8: \"huit\",\n",
        "            9: \"neuf\"\n",
        "        }\n",
        "\n",
        "        self.teens = {\n",
        "            10: \"dix\",\n",
        "            11: \"onze\",\n",
        "            12: \"douze\",\n",
        "            13: \"treize\",\n",
        "            14: \"quatorze\",\n",
        "            15: \"quinze\",\n",
        "            16: \"seize\",\n",
        "            17: \"dix-sept\",\n",
        "            18: \"dix-huit\",\n",
        "            19: \"dix-neuf\"\n",
        "        }\n",
        "\n",
        "        self.tens = {\n",
        "            2: \"vingt\",\n",
        "            3: \"trente\",\n",
        "            4: \"quarante\",\n",
        "            5: \"cinquante\",\n",
        "            6: \"soixante\",\n",
        "            7: \"soixante-dix\",\n",
        "            8: \"quatre-vingt\",\n",
        "            9: \"quatre-vingt-dix\"\n",
        "        }\n",
        "\n",
        "        self.multipliers = {\n",
        "            100: \"cent\",\n",
        "            1_000: \"mille\",\n",
        "            1_000_000: \"million\",\n",
        "            1_000_000_000: \"milliard\",\n",
        "            1_000_000_000_000: \"billion\"\n",
        "        }\n",
        "\n",
        "        # Special case words for French numbers\n",
        "        self.special_tens = {\n",
        "            71: \"soixante et onze\",\n",
        "            72: \"soixante-douze\",\n",
        "            73: \"soixante-treize\",\n",
        "            74: \"soixante-quatorze\",\n",
        "            75: \"soixante-quinze\",\n",
        "            76: \"soixante-seize\",\n",
        "            77: \"soixante-dix-sept\",\n",
        "            78: \"soixante-dix-huit\",\n",
        "            79: \"soixante-dix-neuf\",\n",
        "            91: \"quatre-vingt-onze\",\n",
        "            92: \"quatre-vingt-douze\",\n",
        "            93: \"quatre-vingt-treize\",\n",
        "            94: \"quatre-vingt-quatorze\",\n",
        "            95: \"quatre-vingt-quinze\",\n",
        "            96: \"quatre-vingt-seize\",\n",
        "            97: \"quatre-vingt-dix-sept\",\n",
        "            98: \"quatre-vingt-dix-huit\",\n",
        "            99: \"quatre-vingt-dix-neuf\"\n",
        "        }\n",
        "\n",
        "        # Sorted multipliers from largest to smallest for processing\n",
        "        self.sorted_multipliers = sorted(self.multipliers.keys(), reverse=True)\n",
        "\n",
        "        self.ordinal_suffixes = {\n",
        "            \"1\": \"er\",  # premier\n",
        "            \"default\": \"ème\"  # deuxième, troisième, etc.\n",
        "        }\n",
        "\n",
        "        self.ordinal_words = {\n",
        "            1: \"premier\",\n",
        "            2: \"deuxième\",\n",
        "            3: \"troisième\",\n",
        "            4: \"quatrième\",\n",
        "            5: \"cinquième\"\n",
        "        }\n",
        "\n",
        "        self.currency_symbols = {\n",
        "            \"$\": \"dollar\",\n",
        "            \"£\": \"livre\",\n",
        "            \"€\": \"euro\",\n",
        "            \"¢\": \"centime\"\n",
        "        }\n",
        "\n",
        "        # Update patterns with French-specific regex\n",
        "        self.decimal_pattern = r'\\b\\d+[,.]\\d+\\b'\n",
        "        self.decimal_pattern_method = lambda m: self.convert_decimal(m.group(0))\n",
        "        self.ordinal_pattern = r'\\b(\\d+)(er|ème|e)\\b'\n",
        "        self.currency_pattern = r'(?:([$€£])(\\d+(?:[,.]\\d+)?)|(\\d+(?:[,.]\\d+)?)([€£$]))'\n",
        "        self.cents_pattern = r'¢(\\d+)'\n",
        "\n",
        "    def convert_number(self, num: int) -> str:\n",
        "        \"\"\"Convert a number to its word representation in French\"\"\"\n",
        "        if num == 0:\n",
        "            return \"zéro\"\n",
        "\n",
        "        if num < 0:\n",
        "            return \"moins \" + self.convert_number(abs(num))\n",
        "\n",
        "        if num < 10:\n",
        "            return self.digits[num]\n",
        "\n",
        "        if num < 20:\n",
        "            return self.teens[num]\n",
        "\n",
        "        if num < 100:\n",
        "            # Special cases\n",
        "            if num in self.special_tens:\n",
        "                return self.special_tens[num]\n",
        "\n",
        "            tens_digit = num // 10\n",
        "            units_digit = num % 10\n",
        "\n",
        "            if units_digit == 0:\n",
        "                # Special case for 80\n",
        "                if tens_digit == 8:\n",
        "                    return \"quatre-vingts\"\n",
        "                return self.tens[tens_digit]\n",
        "            elif units_digit == 1 and tens_digit not in [7, 9]:\n",
        "                # Special case for numbers like 21, 31, 41, etc. (but not 71, 91)\n",
        "                return f\"{self.tens[tens_digit]} et un\"\n",
        "            else:\n",
        "                return f\"{self.tens[tens_digit]}-{self.digits[units_digit]}\"\n",
        "\n",
        "        # Handle larger numbers recursively\n",
        "        for multiplier in self.sorted_multipliers:\n",
        "            if num >= multiplier:\n",
        "                quotient, remainder = divmod(num, multiplier)\n",
        "\n",
        "                # Special cases for French numbers\n",
        "                if multiplier == 100:\n",
        "                    if quotient == 1 and remainder == 0:\n",
        "                        return \"cent\"\n",
        "                    elif quotient == 1:\n",
        "                        return f\"cent {self.convert_number(remainder)}\"\n",
        "                    elif remainder == 0:\n",
        "                        return f\"{self.convert_number(quotient)} cents\"\n",
        "                    else:\n",
        "                        return f\"{self.convert_number(quotient)} cent {self.convert_number(remainder)}\"\n",
        "\n",
        "                elif multiplier == 1000:\n",
        "                    if quotient == 1:\n",
        "                        if remainder == 0:\n",
        "                            return \"mille\"\n",
        "                        else:\n",
        "                            return f\"mille {self.convert_number(remainder)}\"\n",
        "                    else:\n",
        "                        if remainder == 0:\n",
        "                            return f\"{self.convert_number(quotient)} mille\"\n",
        "                        else:\n",
        "                            return f\"{self.convert_number(quotient)} mille {self.convert_number(remainder)}\"\n",
        "\n",
        "                else:  # million, milliard, etc.\n",
        "                    plural_suffix = \"s\" if quotient > 1 else \"\"\n",
        "\n",
        "                    if remainder == 0:\n",
        "                        return f\"{self.convert_number(quotient)} {self.multipliers[multiplier]}{plural_suffix}\"\n",
        "                    else:\n",
        "                        return f\"{self.convert_number(quotient)} {self.multipliers[multiplier]}{plural_suffix} {self.convert_number(remainder)}\"\n",
        "\n",
        "        # Should never reach here given the checks above\n",
        "        return str(num)\n",
        "\n",
        "    def convert_decimal(self, num: Union[float, str]) -> str:\n",
        "        \"\"\"Convert a decimal number to its word representation in English\"\"\"\n",
        "        # Ensure num is a float\n",
        "        if isinstance(num, str):\n",
        "            # Replace comma with period for float conversion\n",
        "            num_str = num.replace(',', '.')\n",
        "            num = float(num_str)\n",
        "\n",
        "        # Handle scientific notation by converting to regular float string\n",
        "        num_str = str(num)\n",
        "\n",
        "        if 'e' in num_str.lower():\n",
        "            # For scientific notation, convert to regular notation\n",
        "            num_str = f\"{num:.10f}\".rstrip('0').rstrip('.')\n",
        "\n",
        "        # Split on the decimal point\n",
        "        parts = num_str.split('.')\n",
        "        integer_part = int(parts[0])\n",
        "\n",
        "        integer_words = self.convert_number(integer_part)\n",
        "\n",
        "        if len(parts) > 1:\n",
        "            # Handle the decimal part digit by digit\n",
        "            decimal_part = parts[1]\n",
        "            decimal_words = \" \".join(self.digits[int(digit)] for digit in decimal_part)\n",
        "            return f\"{integer_words} point {decimal_words}\"\n",
        "        else:\n",
        "            # No decimal part\n",
        "            return integer_words\n",
        "\n",
        "    def convert_ordinal(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert an ordinal number to its word representation in French\"\"\"\n",
        "        number = int(match.group(1))\n",
        "\n",
        "        # Special cases\n",
        "        if number in self.ordinal_words:\n",
        "            return self.ordinal_words[number]\n",
        "\n",
        "        # General rule: use the number word + ième\n",
        "        base_word = self.convert_number(number)\n",
        "\n",
        "        if base_word.endswith('e'):\n",
        "            return f\"{base_word[:-1]}ième\"\n",
        "        else:\n",
        "            return f\"{base_word}ième\"\n",
        "\n",
        "    def convert_currency(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert a currency amount to its word representation in French\"\"\"\n",
        "        # Handle both pattern formats (symbol before or after amount)\n",
        "        if match.group(1) is not None:\n",
        "            # Format: $42,50\n",
        "            symbol = match.group(1)\n",
        "            amount = match.group(2).replace(',', '.')\n",
        "        else:\n",
        "            # Format: 42,50€\n",
        "            symbol = match.group(4)\n",
        "            amount = match.group(3).replace(',', '.')\n",
        "\n",
        "        if '.' in amount:\n",
        "            dollars, cents = amount.split('.')\n",
        "            dollars_int = int(dollars)\n",
        "            cents_int = int(cents)\n",
        "\n",
        "            dollars_text = self.convert_number(dollars_int)\n",
        "\n",
        "            if cents_int == 0:\n",
        "                # Handle plural\n",
        "                if dollars_int == 1:\n",
        "                    return f\"un {self.currency_symbols[symbol]}\"\n",
        "                else:\n",
        "                    return f\"{dollars_text} {self.currency_symbols[symbol]}s\"\n",
        "\n",
        "            cents_text = self.convert_number(cents_int)\n",
        "\n",
        "            if dollars_int == 1:\n",
        "                return f\"un {self.currency_symbols[symbol]} et {cents_text} centimes\"\n",
        "            else:\n",
        "                return f\"{dollars_text} {self.currency_symbols[symbol]}s et {cents_text} centimes\"\n",
        "        else:\n",
        "            amount_int = int(amount)\n",
        "            amount_text = self.convert_number(amount_int)\n",
        "\n",
        "            if amount_int == 1:\n",
        "                return f\"un {self.currency_symbols[symbol]}\"\n",
        "            else:\n",
        "                return f\"{amount_text} {self.currency_symbols[symbol]}s\"\n",
        "\n",
        "    def convert_percentage(self, match: re.Match) -> str:\n",
        "        \"\"\"Convert a percentage to its word representation in French\"\"\"\n",
        "        value = match.group(1)\n",
        "\n",
        "        if '.' in value or ',' in value:\n",
        "            value = value.replace(',', '.')\n",
        "            return f\"{self.convert_decimal(float(value))} pour cent\"\n",
        "        else:\n",
        "            return f\"{self.convert_number(int(value))} pour cent\"\n",
        "\n",
        "class ParallelTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor for parallel text data in MT format\n",
        "    Ensures that filtering is applied consistently across language pairs\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Compile regex patterns once\n",
        "        self.non_latin_pattern = re.compile(r'[\\u4e00-\\u9fff\\u0400-\\u04FF\\u0370-\\u03FF\\u3040-\\u30FF]')\n",
        "        self.double_dash_pattern = re.compile(r'--')\n",
        "\n",
        "    def filter_lines(self, files_dict: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Filter lines consistently across parallel files\n",
        "\n",
        "        Args:\n",
        "            files_dict: Dictionary mapping language codes to lists of text lines\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with filtered lines\n",
        "        \"\"\"\n",
        "        result = {lang: [] for lang in files_dict.keys()}\n",
        "\n",
        "        # Get the number of lines (should be the same for all files)\n",
        "        num_lines = len(next(iter(files_dict.values())))\n",
        "\n",
        "        # Check if all files have the same number of lines\n",
        "        if not all(len(lines) == num_lines for lines in files_dict.values()):\n",
        "            raise ValueError(\"All parallel files must have the same number of lines\")\n",
        "\n",
        "        # Process line by line across all files\n",
        "        for i in range(num_lines):\n",
        "            # Check if any line should be filtered out\n",
        "            should_filter = False\n",
        "\n",
        "            for lang, lines in files_dict.items():\n",
        "                line = lines[i]\n",
        "                # Check for non-Latin characters or double dashes\n",
        "                if self.non_latin_pattern.search(line) or self.double_dash_pattern.search(line):\n",
        "                    should_filter = True\n",
        "                    break\n",
        "\n",
        "            # If no filter condition was met, add lines to result\n",
        "            if not should_filter:\n",
        "                for lang, lines in files_dict.items():\n",
        "                    result[lang].append(lines[i])\n",
        "\n",
        "        return result\n",
        "\n",
        "    def preprocess_files(self, file_paths: Dict[str, str]) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Preprocess multiple parallel files\n",
        "\n",
        "        Args:\n",
        "            file_paths: Dictionary mapping language codes to file paths\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processed lines\n",
        "        \"\"\"\n",
        "        # Read all files\n",
        "        files_dict = {}\n",
        "        for lang, path in file_paths.items():\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                files_dict[lang] = f.read().splitlines()\n",
        "\n",
        "        # Apply filtering\n",
        "        return self.filter_lines(files_dict)\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess a single text (removes parentheses and brackets)\n",
        "\n",
        "        Args:\n",
        "            text: Text to preprocess\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed text\n",
        "        \"\"\"\n",
        "        # First remove content within parentheses - handles nested parentheses\n",
        "        while re.search(r'\\([^()]*\\)', text):\n",
        "            text = re.sub(r'\\([^()]*\\)', '', text)\n",
        "\n",
        "        # Then remove content within brackets - handles nested brackets\n",
        "        while re.search(r'\\[[^\\[\\]]*\\]', text):\n",
        "            text = re.sub(r'\\[[^\\[\\]]*\\]', '', text)\n",
        "\n",
        "        # Remove any remaining unpaired parentheses and brackets\n",
        "        text = re.sub(r'[\\(\\)\\[\\]]', '', text)\n",
        "\n",
        "        # remove « and »\n",
        "        text = re.sub(r'«|»', '', text)\n",
        "\n",
        "        # strip leading and trailing whitespace\n",
        "        text = text.strip()\n",
        "\n",
        "        # ensure the first letter is capitalized\n",
        "        if text:\n",
        "            text = text[0].upper() + text[1:]\n",
        "\n",
        "        # replace commas at the end of a sentence with fullstop\n",
        "        if text.endswith(','):\n",
        "            text = text[:-1] + '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    def normalize(self, text: str, language: str = 'en') -> str:\n",
        "        \"\"\"\n",
        "        Perform inverse normalization on text in the specified language\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to inverse normalize\n",
        "            language (str): Language code ('en' for English, 'fr' for French)\n",
        "\n",
        "        Returns:\n",
        "            str: Inverse normalized text\n",
        "        \"\"\"\n",
        "        if language not in self.normalizers:\n",
        "            raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "        # Apply common preprocessing\n",
        "        text = self.preprocess(text)\n",
        "\n",
        "        # Apply language-specific normalization\n",
        "        return self.normalizers[language](text)\n",
        "\n",
        "    def __call__(self, text: str, language: str = 'en') -> str:\n",
        "        \"\"\"\n",
        "        Perform inverse normalization on text in the specified language\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to inverse normalize\n",
        "            language (str): Language code ('en' for English, 'fr' for French)\n",
        "\n",
        "        Returns:\n",
        "            str: Inverse normalized text\n",
        "        \"\"\"\n",
        "        return self.normalize(text, language)\n",
        "\n",
        "class TextLengthFilter:\n",
        "    \"\"\"\n",
        "    Filter texts based on length criteria\n",
        "    \"\"\"\n",
        "    def __init__(self, min_length: int = 4, max_length: int = 500):\n",
        "        \"\"\"\n",
        "        Initialize the filter with length constraints\n",
        "\n",
        "        Args:\n",
        "            min_length: Minimum allowed text length (inclusive)\n",
        "            max_length: Maximum allowed text length (inclusive)\n",
        "        \"\"\"\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def filter_text(self, text: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Filter a single text based on length criteria\n",
        "\n",
        "        Args:\n",
        "            text: Text to filter\n",
        "\n",
        "        Returns:\n",
        "            The original text if it meets the criteria, None otherwise\n",
        "        \"\"\"\n",
        "        text_length = len(text)\n",
        "        if self.min_length <= text_length <= self.max_length:\n",
        "            return text\n",
        "        return None\n",
        "\n",
        "    def filter_texts(self, texts: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Filter a list of texts based on length criteria\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to filter\n",
        "\n",
        "        Returns:\n",
        "            List of texts that meet the criteria\n",
        "        \"\"\"\n",
        "        return [text for text in texts if self.filter_text(text) is not None]\n",
        "\n",
        "    def filter_files(self, file_paths: List[str], encoding: str = 'utf-8') -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Filter multiple files line by line\n",
        "\n",
        "        Args:\n",
        "            file_paths: List of file paths to process\n",
        "            encoding: File encoding (default: utf-8)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping file paths to lists of filtered lines\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        for path in file_paths:\n",
        "            with open(path, 'r', encoding=encoding) as f:\n",
        "                lines = f.read().splitlines()\n",
        "                result[path] = self.filter_texts(lines)\n",
        "        return result\n",
        "\n",
        "    def filter_parallel_texts(self, texts_dict: Dict[str, List[str]]) -> Tuple[Dict[str, List[str]], Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Filter texts consistently across parallel data\n",
        "\n",
        "        Args:\n",
        "            texts_dict: Dictionary mapping languages to lists of texts\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "                - Dictionary with filtered texts (maintaining alignment)\n",
        "                - Dictionary with retention rates for each language\n",
        "        \"\"\"\n",
        "        result = {lang: [] for lang in texts_dict.keys()}\n",
        "\n",
        "        # Get number of texts (should be the same for all languages)\n",
        "        num_texts = len(next(iter(texts_dict.values())))\n",
        "\n",
        "        # Check if all languages have the same number of texts\n",
        "        if not all(len(texts) == num_texts for texts in texts_dict.values()):\n",
        "            raise ValueError(\"All parallel texts must have the same number of items\")\n",
        "\n",
        "        # Track how many texts are kept for each language\n",
        "        kept_count = 0\n",
        "\n",
        "        # Process text by text across all languages\n",
        "        for i in range(num_texts):\n",
        "            # Check if any text should be filtered out based on length\n",
        "            should_keep = True\n",
        "\n",
        "            for lang, texts in texts_dict.items():\n",
        "                text = texts[i]\n",
        "                text_length = len(text)\n",
        "                if text_length < self.min_length or text_length > self.max_length:\n",
        "                    should_keep = False\n",
        "                    break\n",
        "\n",
        "            # If all texts pass the length filter, keep them\n",
        "            if should_keep:\n",
        "                kept_count += 1\n",
        "                for lang, texts in texts_dict.items():\n",
        "                    result[lang].append(texts[i])\n",
        "\n",
        "        # Calculate retention rates\n",
        "        retention_rates = {\n",
        "            lang: (kept_count / num_texts) * 100 if num_texts > 0 else 100.0\n",
        "            for lang in texts_dict.keys()\n",
        "        }\n",
        "\n",
        "        print(f\"Texts retained for each language: {retention_rates}\")\n",
        "\n",
        "        return result, retention_rates\n",
        "\n",
        "    def __call__(self, text: Union[str, List[str], Dict[str, List[str]]], return_stats: bool = False) -> Union[\n",
        "        Optional[str],\n",
        "        List[str],\n",
        "        Dict[str, List[str]],\n",
        "        Tuple[Dict[str, List[str]], Dict[str, float]]\n",
        "    ]:\n",
        "        \"\"\"\n",
        "        Filter text based on its type\n",
        "\n",
        "        Args:\n",
        "            text: Single text, list of texts, or dictionary of parallel texts\n",
        "            return_stats: Whether to return statistics (only applicable for dictionary input)\n",
        "\n",
        "        Returns:\n",
        "            Filtered text(s) of the same type as input, with optional stats\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            return self.filter_text(text)\n",
        "        elif isinstance(text, list):\n",
        "            return self.filter_texts(text)\n",
        "        elif isinstance(text, dict):\n",
        "            result = self.filter_parallel_texts(text)\n",
        "            return result if return_stats else result[0]\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported input type: {type(text)}\")\n",
        "\n",
        "\n",
        "# Integration with existing code\n",
        "class EnhancedMultilingualInverseTextNormalizer:\n",
        "    \"\"\"\n",
        "    Enhanced inverse text normalizer with length filtering capabilities\n",
        "    \"\"\"\n",
        "    def __init__(self, min_length: int = 4, max_length: int = 500):\n",
        "        self.normalizers = {\n",
        "            'en': EnglishInverseNumberNormalizer(),\n",
        "            'fr': FrenchInverseNumberNormalizer()\n",
        "        }\n",
        "        self.preprocessor = ParallelTextPreprocessor()\n",
        "        self.length_filter = TextLengthFilter(min_length, max_length)\n",
        "\n",
        "    def normalize_and_filter(self, text: str, language: str = 'en') -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Normalize text and filter based on length criteria\n",
        "\n",
        "        Args:\n",
        "            text: Text to normalize and filter\n",
        "            language: Language code ('en' for English, 'fr' for French)\n",
        "\n",
        "        Returns:\n",
        "            Normalized text if it meets length criteria, None otherwise\n",
        "        \"\"\"\n",
        "        if language not in self.normalizers:\n",
        "            raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "        # Apply preprocessing\n",
        "        text = self.preprocessor.preprocess_text(text)\n",
        "\n",
        "        # Skip normalization if text doesn't meet length criteria\n",
        "        if not self.length_filter.filter_text(text):\n",
        "            return None\n",
        "\n",
        "        # Apply language-specific normalization\n",
        "        normalized = self.normalizers[language](text)\n",
        "\n",
        "        # Apply length filtering to normalized text\n",
        "        return self.length_filter.filter_text(normalized)\n",
        "\n",
        "    def normalize_and_filter_batch(self, texts: List[str], language: str = 'en') -> List[str]:\n",
        "        \"\"\"\n",
        "        Normalize and filter a batch of texts\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to normalize and filter\n",
        "            language: Language code\n",
        "\n",
        "        Returns:\n",
        "            List of normalized texts that meet length criteria\n",
        "        \"\"\"\n",
        "        return [\n",
        "            normalized for text in texts\n",
        "            if (normalized := self.normalize_and_filter(text, language)) is not None\n",
        "        ]\n",
        "\n",
        "    def normalize_and_filter_parallel(self, texts_dict: Dict[str, List[str]], return_stats: bool = False):\n",
        "        # Store initial counts\n",
        "        initial_counts = {lang: len(texts) for lang, texts in texts_dict.items()}\n",
        "\n",
        "        # Apply first round of filtering\n",
        "        texts_dict = self.preprocessor.filter_lines(texts_dict)\n",
        "\n",
        "        processed_dict = {}\n",
        "        for lang, texts in texts_dict.items():\n",
        "            processed_dict[lang] = [self.preprocessor.preprocess_text(text) for text in texts]\n",
        "\n",
        "        filtered_dict = self.length_filter.filter_parallel_texts(processed_dict)[0]\n",
        "\n",
        "        # Apply normalization\n",
        "        normalized_dict = {}\n",
        "        for lang, texts in filtered_dict.items():\n",
        "            normalized_dict[lang] = [\n",
        "                self.normalizers[lang](text) for text in tqdm(texts, desc=f\"Normalizing {lang}\")\n",
        "            ]\n",
        "\n",
        "        # Final filtering\n",
        "        final_filtered, _ = self.length_filter.filter_parallel_texts(normalized_dict)\n",
        "\n",
        "        # Calculate true retention rates based on initial counts\n",
        "        final_retention_rates = {\n",
        "            lang: (len(final_filtered[lang]) / initial_counts[lang]) * 100\n",
        "            for lang in initial_counts\n",
        "        }\n",
        "\n",
        "        if return_stats:\n",
        "            return final_filtered, final_retention_rates\n",
        "        return final_filtered\n",
        "\n",
        "    def __call__(self, text: Union[str, List[str], Dict[str, List[str]]], language: str = 'en', return_stats: bool = False) -> Union[\n",
        "        Optional[str],\n",
        "        List[str],\n",
        "        Dict[str, List[str]],\n",
        "        Tuple[Dict[str, List[str]], Dict[str, float]]\n",
        "    ]:\n",
        "        \"\"\"\n",
        "        Normalize and filter based on input type\n",
        "\n",
        "        Args:\n",
        "            text: Single text, list of texts, or dictionary of parallel texts\n",
        "            language: Language code (ignored for dictionary input)\n",
        "            return_stats: Whether to return retention statistics (only for dictionary input)\n",
        "\n",
        "        Returns:\n",
        "            Normalized and filtered text(s) of the same type as input, with optional stats\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            return self.normalize_and_filter(text, language)\n",
        "        elif isinstance(text, list):\n",
        "            return self.normalize_and_filter_batch(text, language)\n",
        "        elif isinstance(text, dict):\n",
        "            return self.normalize_and_filter_parallel(text, return_stats)\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported input type: {type(text)}\")\n",
        "\n",
        "\n",
        "def test_text_length_filter():\n",
        "    # Create test texts\n",
        "    test_texts = [\n",
        "        \"abc\",               # Too short\n",
        "        \"hello\",             # OK\n",
        "        \"This is a test.\",   # OK\n",
        "        \"A\" * 501            # Too long\n",
        "    ]\n",
        "\n",
        "    # Create filter\n",
        "    text_filter = TextLengthFilter(min_length=4, max_length=500)\n",
        "\n",
        "    # Test single text filtering\n",
        "    assert text_filter.filter_text(\"abc\") is None\n",
        "    assert text_filter.filter_text(\"hello\") == \"hello\"\n",
        "    assert text_filter.filter_text(\"A\" * 501) is None\n",
        "\n",
        "    # Test batch filtering\n",
        "    filtered = text_filter.filter_texts(test_texts)\n",
        "    assert len(filtered) == 2\n",
        "    assert \"abc\" not in filtered\n",
        "    assert \"hello\" in filtered\n",
        "    assert \"This is a test.\" in filtered\n",
        "    assert \"A\" * 501 not in filtered\n",
        "\n",
        "    # Test parallel text filtering\n",
        "    parallel_texts = {\n",
        "        'en': [\"abc\", \"hello\", \"This is a test.\", \"A\" * 501],\n",
        "        'fr': [\"def\", \"bonjour\", \"C'est un test.\", \"B\" * 501]\n",
        "    }\n",
        "\n",
        "    filtered_parallel, stats = text_filter.filter_parallel_texts(parallel_texts)\n",
        "    assert len(filtered_parallel['en']) == 2\n",
        "    assert len(filtered_parallel['fr']) == 2\n",
        "    assert filtered_parallel['en'] == [\"hello\", \"This is a test.\"]\n",
        "    assert filtered_parallel['fr'] == [\"bonjour\", \"C'est un test.\"]\n",
        "\n",
        "    print(\"All text length filter tests passed!\")\n",
        "\n",
        "\n",
        "def test_integration():\n",
        "    # Create test texts\n",
        "    test_texts = {\n",
        "        'en': [\n",
        "            \"123\",                     # OK but short\n",
        "            \"The price is $42.50\",     # OK\n",
        "            \"A\" * 501,                 # Too long\n",
        "            \"Chapter 10\",                # OK\n",
        "            \"The House rose (and observed a minute' s silence)\"\n",
        "        ],\n",
        "        'fr': [\n",
        "            \"123\",                     # OK but short\n",
        "            \"Le prix est 42,50€\",      # OK\n",
        "            \"B\" * 501,                 # Too long\n",
        "            \"Chapitre 10\",               # OK\n",
        "            \"Le Parlement, debout(, observe une minute de silence)\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Create normalizer with filtering\n",
        "    normalizer = EnhancedMultilingualInverseTextNormalizer(min_length=4, max_length=500)\n",
        "\n",
        "    # Test single text normalization and filtering\n",
        "    assert normalizer(\"123\", 'en') is None  # Too short\n",
        "    assert normalizer(\"The price is $42.50\", 'en') == \"The price is forty-two dollars and fifty cents\"\n",
        "    assert normalizer(\"A\" * 501, 'en') is None  # Too long\n",
        "\n",
        "    # Test parallel normalization and filtering with stats\n",
        "    result, stats = normalizer(test_texts, return_stats=True)\n",
        "    print(result)\n",
        "    assert len(result['en']) == 3\n",
        "    assert \"one hundred and twenty-three\" not in result['en']  # Filtered out (orig text too short)\n",
        "    assert \"The price is forty-two dollars and fifty cents\" in result['en']\n",
        "    assert \"Chapter ten\" in result['en']\n",
        "    assert \"The House rose\" in result['en']\n",
        "\n",
        "    assert len(result['fr']) == 3\n",
        "    assert \"cent vingt-trois\" not in result['fr']  # Filtered out (orig text too short)\n",
        "    assert \"Le prix est quarante-deux euros et cinquante centimes\" in result['fr']\n",
        "    assert \"Chapitre dix\" in result['fr']\n",
        "    assert \"Le Parlement, debout\" in result['fr']\n",
        "\n",
        "    print(stats)\n",
        "    # Check retention rates\n",
        "    assert stats['en'] == 60.0  # 3 out of 5 texts retained\n",
        "    assert stats['fr'] == 60.0  # 3 out of 5 texts retained\n",
        "\n",
        "    print(\"All integration tests passed!\")\n",
        "    print(f\"Retention rate: {stats['en']}% of texts were retained\")\n",
        "\n",
        "\n",
        "def process_parallel_corpus(file_paths, min_length=4, max_length=500, output_dir=None):\n",
        "    \"\"\"\n",
        "    Process a parallel corpus with length filtering and report statistics\n",
        "\n",
        "    Args:\n",
        "        file_paths: Dictionary mapping language codes to file paths\n",
        "        min_length: Minimum text length to keep\n",
        "        max_length: Maximum text length to keep\n",
        "        output_dir: Directory to write filtered files (if None, don't write files)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of filtered texts and retention statistics\n",
        "    \"\"\"\n",
        "    # Initialize filter and normalizer\n",
        "    length_filter = TextLengthFilter(min_length, max_length)\n",
        "    normalizer = EnhancedMultilingualInverseTextNormalizer(min_length, max_length)\n",
        "\n",
        "    # Read all files\n",
        "    texts_dict = {}\n",
        "    for lang, path in file_paths.items():\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            texts_dict[lang] = f.read().splitlines()\n",
        "\n",
        "    # Get initial text counts\n",
        "    initial_counts = {lang: len(texts) for lang, texts in texts_dict.items()}\n",
        "\n",
        "    # Filter and normalize\n",
        "    filtered_texts, retention_stats = normalizer(texts_dict, return_stats=True)\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"=== Parallel Corpus Processing Report ===\")\n",
        "    print(f\"Minimum length: {min_length}, Maximum length: {max_length}\")\n",
        "    print(\"\\nRetention Statistics:\")\n",
        "    for lang, rate in retention_stats.items():\n",
        "        initial = initial_counts[lang]\n",
        "        retained = len(filtered_texts[lang])\n",
        "        print(f\"  {lang}: {retained}/{initial} texts retained ({rate:.2f}%)\")\n",
        "\n",
        "    # Write output files if requested\n",
        "    if output_dir:\n",
        "        import os\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        for lang, texts in filtered_texts.items():\n",
        "            output_path = os.path.join(output_dir, f\"filtered_{lang}.txt\")\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write('\\n'.join(texts))\n",
        "            print(f\"Wrote filtered texts to {output_path}\")\n",
        "\n",
        "    return filtered_texts, retention_stats\n",
        "\n",
        "\n",
        "test_text_length_filter()\n",
        "test_integration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUDhXQ2OgCQ",
        "outputId": "8c0b48dd-3d41-408d-9a7a-db16aaaa6204"
      },
      "outputs": [],
      "source": [
        "process_parallel_corpus({\n",
        "    'en': '/content/Europarl/Europarl.en-fr.en',\n",
        "    'fr': '/content/Europarl/Europarl.en-fr.fr'\n",
        "}, output_dir='processed_europarl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1FqiUj5gWID",
        "outputId": "9d10c0b4-8b89-4d99-cae2-c36f66f9ba9b"
      },
      "outputs": [],
      "source": [
        "process_parallel_corpus({\n",
        "    'en': '/content/TED/TED2020.en-fr.en',\n",
        "    'fr': '/content/TED/TED2020.en-fr.fr'\n",
        "}, output_dir='processed_ted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sM9VgWJR6DZ",
        "outputId": "e4f33ecb-d3ae-497f-fb68-8d74b47d8a83"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "\n",
        "def sample_sentences(texts_dict: Dict[str, List[str]], n: int = 100, seed: int = 42) -> Tuple[Dict[str, List[str]], List[int]]:\n",
        "    \"\"\"\n",
        "    Sample n sentences from each language in the texts dictionary.\n",
        "\n",
        "    Args:\n",
        "        texts_dict: Dictionary mapping language codes to lists of texts\n",
        "        n: Number of sentences to sample\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - Dictionary with sampled texts\n",
        "            - List of indices of the sampled texts\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Get the number of texts (should be the same for all languages)\n",
        "    num_texts = len(next(iter(texts_dict.values())))\n",
        "\n",
        "    # Generate n random indices without replacement\n",
        "    if n > num_texts:\n",
        "        print(f\"Warning: Requested sample size {n} is larger than available texts ({num_texts})\")\n",
        "        n = num_texts\n",
        "\n",
        "    sample_indices = sorted(random.sample(range(num_texts), n))\n",
        "\n",
        "    # Sample texts for each language\n",
        "    sampled_texts = {}\n",
        "    for lang, texts in texts_dict.items():\n",
        "        sampled_texts[lang] = [texts[i] for i in sample_indices]\n",
        "\n",
        "    return sampled_texts, sample_indices\n",
        "\n",
        "\n",
        "def analyze_lengths(texts_dict: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Analyze the length distribution of texts for each language.\n",
        "\n",
        "    Args:\n",
        "        texts_dict: Dictionary mapping language codes to lists of texts\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with length statistics for each language\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for lang, texts in texts_dict.items():\n",
        "        # Calculate lengths\n",
        "        lengths = [len(text) for text in texts]\n",
        "\n",
        "        # Calculate statistics\n",
        "        stats = {\n",
        "            'min': min(lengths),\n",
        "            'max': max(lengths),\n",
        "            'mean': sum(lengths) / len(lengths),\n",
        "            'median': sorted(lengths)[len(lengths) // 2],\n",
        "            'std': np.std(lengths),\n",
        "            'count': len(lengths)\n",
        "        }\n",
        "\n",
        "        # Bin lengths in groups of n\n",
        "        bins = {}\n",
        "        n = 20\n",
        "        for length in lengths:\n",
        "            bin_key = f\"{(length // n) * n}-{(length // n) * n + n-1}\"\n",
        "            bins[bin_key] = bins.get(bin_key, 0) + 1\n",
        "\n",
        "        stats['bins'] = bins\n",
        "        results[lang] = stats\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Main function to load files, sample sentences, and analyze lengths\n",
        "def main(file_paths: Dict[str, str], n_samples: int = 100, save_indices: bool = True, seed:int = 0, json_path: str = None):\n",
        "    \"\"\"\n",
        "    Load text files, sample sentences, analyze lengths and save results\n",
        "\n",
        "    Args:\n",
        "        file_paths: Dictionary mapping language codes to file paths\n",
        "        n_samples: Number of sentences to sample\n",
        "        save_indices: Whether to save indices to a file\n",
        "    \"\"\"\n",
        "    # Load all files\n",
        "    texts_dict = {}\n",
        "    for lang, path in file_paths.items():\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            texts_dict[lang] = f.read().splitlines()\n",
        "        print(f\"Loaded {len(texts_dict[lang])} lines from {path}\")\n",
        "\n",
        "    # Sample sentences\n",
        "    sampled_texts, sample_indices = sample_sentences(texts_dict, n_samples, seed=seed)\n",
        "    print(f\"Sampled {len(sample_indices)} sentences\")\n",
        "\n",
        "    # Analyze lengths\n",
        "    stats = analyze_lengths(sampled_texts)\n",
        "\n",
        "    # Save sample indices for future reference\n",
        "    if save_indices:\n",
        "        import json\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(sample_indices, f)\n",
        "        print(f\"Sample indices saved to {json_path}\")\n",
        "\n",
        "    return sampled_texts, sample_indices, stats\n",
        "\n",
        "\n",
        "# Example usage with real files\n",
        "if __name__ == \"__main__\":\n",
        "    # for ted\n",
        "    file_paths = {\n",
        "        'ted_en': \"/content/processed_ted/filtered_en.txt\",\n",
        "        'ted_fr': \"/content/processed_ted/filtered_fr.txt\"\n",
        "    }\n",
        "    seed = 0\n",
        "\n",
        "    # Run the analysis with 100 samples\n",
        "    sampled_texts, sample_indices, stats = main(file_paths, n_samples=25000, json_path='ted.json', seed=seed)\n",
        "    print(stats)\n",
        "    # for europarl\n",
        "    file_paths = {\n",
        "        'europarl_en': \"/content/processed_europarl/filtered_en.txt\",\n",
        "        'europarl_fr': \"/content/processed_europarl/filtered_fr.txt\"}\n",
        "\n",
        "    # Run the analysis with 100 samples\n",
        "    sampled_texts, sample_indices, stats = main(file_paths, n_samples=25000, json_path='europarl.json',seed=seed)\n",
        "    print(stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm7X1Es3s7in",
        "outputId": "eb2cf466-730c-4b0f-fc1a-5456cae659df"
      },
      "outputs": [],
      "source": [
        "!ls -lh /content/processed_ted/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccval72Hs_1j",
        "outputId": "3b67d7a1-127d-4c86-d309-cf17a5be9eef"
      },
      "outputs": [],
      "source": [
        "!ls -lh /content/processed_europarl/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBhE0irTzOH",
        "outputId": "e79c5d3e-4f07-487b-f31a-b75d8ba1428e"
      },
      "outputs": [],
      "source": [
        "! cat /content/processed_europarl/filtered_fr.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ha8UlpPSlTJ",
        "outputId": "7250db26-fa93-4aee-8be9-5f17628526c1"
      },
      "outputs": [],
      "source": [
        "!cat /content/processed_europarl/filtered_en.txt /content/processed_europarl/filtered_en.txt /content/processed_ted/filtered_en.txt /content/processed_ted/filtered_en.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POypso7URgx7"
      },
      "outputs": [],
      "source": [
        "!cat /content/processed_europarl/filtered_en.txt /content/processed_europarl/filtered_en.txt /content/processed_ted/filtered_en.txt /content/processed_ted/filtered_en.txt | grep \"||\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWbOoCsfVBcr"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hq45oE5SQOE",
        "outputId": "d92a38a7-ae94-4b3a-a4b0-1510502cf685"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/processed_europarl/filtered_en.txt\", 'r') as f_en, open(\n",
        "    \"/content/processed_europarl/filtered_fr.txt\", 'r') as f_fr, open(\n",
        "        \"/content/europarl.json\", 'r') as f_json, open(\"/content/europarl.txt\", 'w') as f_out:\n",
        "\n",
        "    europarl_en = f_en.readlines()\n",
        "    europarl_fr = f_fr.readlines()\n",
        "\n",
        "    # Convert JSON list to a set for O(1) lookups\n",
        "    europarl_indices = set(json.load(f_json))\n",
        "\n",
        "    for i, (line_en, line_fr) in tqdm(enumerate(zip(europarl_en, europarl_fr)), total=len(europarl_en)):\n",
        "        if i in europarl_indices:  # O(1) lookup instead of search\n",
        "            f_out.write(f\"{line_en.strip()} || {line_fr.strip()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "corEnP60UZem",
        "outputId": "04660b38-a5e5-490d-a276-358fffdd9be8"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/processed_ted/filtered_en.txt\", 'r') as f_en, open(\n",
        "    \"/content/processed_ted/filtered_fr.txt\", 'r') as f_fr, open(\n",
        "        \"/content/ted.json\", 'r') as f_json, open(\"/content/ted.txt\", 'w') as f_out:\n",
        "\n",
        "    ted_en = f_en.readlines()\n",
        "    ted_fr = f_fr.readlines()\n",
        "\n",
        "    # Convert JSON list to a set for O(1) lookups\n",
        "    ted_indices = set(json.load(f_json))\n",
        "\n",
        "    for i, (line_en, line_fr) in tqdm(enumerate(zip(ted_en, ted_fr)), total=len(ted_en)):\n",
        "        if i in ted_indices:  # O(1) lookup instead of search\n",
        "            f_out.write(f\"{line_en.strip()} || {line_fr.strip()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPPommR8Zria",
        "outputId": "0cbc85cc-aa2f-4bd0-f7e9-2392b7dcb086"
      },
      "outputs": [],
      "source": [
        "!cat europarl.txt ted.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUsAMSTma0YA",
        "outputId": "633491ca-dc56-4806-85bd-24f61f4715a8"
      },
      "outputs": [],
      "source": [
        "! cat europarl.txt > en_fr.txt\n",
        "! cat ted.txt >> en_fr.txt\n",
        "! cat en_fr.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9sQ-wHbcT1",
        "outputId": "d1c6e533-0b11-4eb3-b8fc-9b2da6893ac6"
      },
      "outputs": [],
      "source": [
        "!ls -l en_fr.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgxD7yON92Zb"
      },
      "source": [
        "## Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSmWS2DT93az"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.cloud import translate_v3\n",
        "\n",
        "# Set this to your project ID\n",
        "PROJECT_ID = \"upbeat-nation-454716-d0\"\n",
        "\n",
        "# Path to your service account key file\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/upbeat-nation-454716-d0-4491769993f7.json\"\n",
        "\n",
        "def translate_text(text, target_language=\"fr\", source_language=\"en-US\"):\n",
        "    \"\"\"\n",
        "    Translates text to the target language.\n",
        "\n",
        "    Args:\n",
        "        text: Text to translate\n",
        "        target_language: Language code to translate to\n",
        "        source_language: Language code of the source text\n",
        "\n",
        "    Returns:\n",
        "        TranslateTextResponse containing the translations\n",
        "    \"\"\"\n",
        "    # Initialize client\n",
        "    client = translate_v3.TranslationServiceClient()\n",
        "\n",
        "    # Set parent resource (project and location)\n",
        "    parent = f\"projects/{PROJECT_ID}/locations/global\"\n",
        "\n",
        "    # Call the API\n",
        "    response = client.translate_text(\n",
        "        request={\n",
        "            \"parent\": parent,\n",
        "            \"contents\": text,\n",
        "            \"mime_type\": \"text/plain\",\n",
        "            \"source_language_code\": source_language,\n",
        "            \"target_language_code\": target_language,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    # print(response)\n",
        "    # for translation in response.translations:\n",
        "    #     print(f\"Translated text: {translation}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "texts_to_translate = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"First of all, this is where we're projected to go with the U.S. contribution to global warming, under business as usual.\",\n",
        "    \"Efficiency in end-use electricity and end-use of all energy is the low-hanging fruit.\"\n",
        "]\n",
        "response = translate_text(texts_to_translate, \"fon\", \"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpulEqS2gEBP",
        "outputId": "4d8bf672-4a9d-4ba6-d5bf-3ccbd268fad3"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDeEnGclg_cJ"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ed_AuTu58o",
        "outputId": "1b3b009b-83d0-44ce-bedc-61bd74b3add0"
      },
      "outputs": [],
      "source": [
        "input_file = \"en_fr.txt\"\n",
        "output_file = \"bem_en.txt\"\n",
        "source_lang = \"en\"\n",
        "target_lang = \"bem\"\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Process file in batches\n",
        "with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "    lines = f_in.readlines()\n",
        "    total_batches = (len(lines) + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(lines))\n",
        "\n",
        "        # Extract English text from each line in this batch\n",
        "        batch_texts = [line.strip().split(\" || \")[0] for line in lines[start_idx:end_idx]]\n",
        "\n",
        "        # Translate the batch\n",
        "        translated_texts = translate_text(batch_texts, target_language=target_lang, source_language=source_lang)\n",
        "        # write the translated bem to the output file sentence by sentence as bem || en\n",
        "        for i, translation in enumerate(translated_texts.translations):\n",
        "            f_out.write(f\"{translation.translated_text} || {batch_texts[i]}\\n\")\n",
        "        # sleep for 100ms\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# save to gdrive\n",
        "!cp bem_en.txt /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBPCfGcxkfz8",
        "outputId": "f981a784-4d4b-4ad5-a778-3261aea0b521"
      },
      "outputs": [],
      "source": [
        "# !ls -lh bem_en.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxzWGfkiUWD-",
        "outputId": "63d4e5ac-a2e1-4591-a59f-63d047f1ea15"
      },
      "outputs": [],
      "source": [
        "input_file = \"en_fr.txt\"\n",
        "output_file = \"fon_fr.txt\"\n",
        "source_lang = \"en\"\n",
        "target_lang = \"fon\"\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Process file in batches\n",
        "with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "    lines = f_in.readlines()\n",
        "    total_batches = (len(lines) + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(lines))\n",
        "\n",
        "        # Extract English text from each line in this batch\n",
        "        batch_texts = [line.strip().split(\" || \")[0] for line in lines[start_idx:end_idx]]\n",
        "        french_texts = [line.strip().split(\" || \")[1] for line in lines[start_idx:end_idx]]\n",
        "        # Translate the batch\n",
        "        translated_texts = translate_text(batch_texts, target_language=target_lang, source_language=source_lang)\n",
        "        # write the translated bem to the output file sentence by sentence as fon || fr\n",
        "        for i, translation in enumerate(translated_texts.translations):\n",
        "            f_out.write(f\"{translation.translated_text} || {french_texts[i]}\\n\")\n",
        "        # sleep for 100ms\n",
        "        time.sleep(0.1)\n",
        "\n",
        "# save to gdrive\n",
        "!cp fon_fr.txt /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
