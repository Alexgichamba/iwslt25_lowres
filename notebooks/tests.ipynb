{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2e_st.trainer import STDataset, SpecConfig\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2e_st.text.tokenizer import CustomTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Register as a fast tokenizer in the second parameter\n",
    "AutoTokenizer.register(\"custom\", None, CustomTokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"alexgichamba/iwslt25_uncased_4096\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vocab size\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.bem_lang_token, tokenizer.eng_lang_token, tokenizer.fra_lang_token, tokenizer.fon_lang_token)\n",
    "print(tokenizer.bem_lang_token_id, tokenizer.eng_lang_token_id, tokenizer.fra_lang_token_id, tokenizer.fon_lang_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.\".lower()))\n",
    "print(len(tokenizer.tokenize(\"I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.\".lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"Je vais soumettre également le problème au Collège des questeurs et je suis certaine que nos questeurs auront à cur de faire en sorte que nous respections la réglementation qu' en effet nous votons.\".lower()))\n",
    "print(len(tokenizer.tokenize(\"Je vais soumettre également le problème au Collège des questeurs et je suis certaine que nos questeurs auront à cur de faire en sorte que nous respections la réglementation qu' en effet nous votons.\".lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"Ée yě ɖɔ mɔ̌ ɔ́, Mɔyízi lɛ́ kɔ bó yi ɖɔ nú Mawu Mavɔmavɔ ɖɔ: \\\"Aklúnɔ, étɛ́wú a wa nǔ xá togun élɔ́?\".lower()))\n",
    "print(len(tokenizer.tokenize(\"Ée yě ɖɔ mɔ̌ ɔ́, Mɔyízi lɛ́ kɔ bó yi ɖɔ nú Mawu Mavɔmavɔ ɖɔ: \\\"Aklúnɔ, étɛ́wú a wa nǔ xá togun élɔ́?\".lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"\\\"Pa kuti kasebanya naikila pali imwe, ali ne cipyu cickalamba, pa kwishibo kuti ali ne nshita inono fye.\\\" - Ukusokoloa 12:12.\".lower()))\n",
    "print(len(tokenizer.tokenize(\"\\\"Pa kuti kasebanya naikila pali imwe, ali ne cipyu cickalamba, pa kwishibo kuti ali ne nshita inono fye.\\\" - Ukusokoloa 12:12.\".lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make spec config instance\n",
    "spec_config = SpecConfig(\n",
    "    n_mels=80,\n",
    "    hop_length=256,\n",
    "    n_fft=1024,\n",
    "    sample_rate=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = STDataset(dataset_json=\"../corpora/train.json\",\n",
    "                           tokenizer=tokenizer,\n",
    "                            spec_config=spec_config,\n",
    "                            case_standardization=\"lower\")\n",
    "\n",
    "sample_loader = DataLoader(sample_dataset, batch_size=2, collate_fn=sample_dataset.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in sample_loader:\n",
    "    mels = batch[\"mel\"]\n",
    "    speech_lengths = batch[\"speech_lengths\"]\n",
    "    text_lengths = batch[\"text_lengths\"]\n",
    "    input_tokens = batch[\"input_tokens\"]\n",
    "    st_target_tokens = batch[\"st_target_tokens\"]\n",
    "    asr_target_tokens = batch[\"asr_target_tokens\"]\n",
    "\n",
    "    print(f\"mel shape: {mels.shape}\")\n",
    "    print(f\"speech lengths: {speech_lengths}\")\n",
    "    print(f\"text lengths: {text_lengths}\")\n",
    "\n",
    "    for i in range(len(input_tokens)):\n",
    "        print(f\"input: {tokenizer.decode(input_tokens[i])}\")\n",
    "        print(f\"st target: {tokenizer.decode(st_target_tokens[i])}\")\n",
    "        print(f\"asr target: {tokenizer.decode(asr_target_tokens[i])}\")\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
