{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2e_st.trainer import STDataset, SpecConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from e2e_st.utils.attention_masks import key_padding_mask, causal_mask\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "print(\"Available backends:\", torchaudio.list_audio_backends())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2e_st.text.tokenizer import load_custom_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_custom_tokenizer(\"alexgichamba/iwslt25_uncased_16384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vocab size\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.bem_lang_token, tokenizer.eng_lang_token, tokenizer.fra_lang_token, tokenizer.fon_lang_token)\n",
    "print(tokenizer.bem_lang_token_id, tokenizer.eng_lang_token_id, tokenizer.fra_lang_token_id, tokenizer.fon_lang_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.\".lower()\n",
    "print(tokenizer.tokenize(english_text))\n",
    "print(tokenizer.decode(tokenizer.encode(english_text)))\n",
    "assert tokenizer.decode(tokenizer.encode(english_text)) == english_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_text = \"Je vais soumettre également le problème au Collège des questeurs et je suis certaine que nos questeurs auront à cur de faire en sorte que nous respections la réglementation qu' en effet nous votons.\".lower()\n",
    "print(tokenizer.tokenize(french_text))\n",
    "assert tokenizer.decode(tokenizer.encode(french_text)) == french_text\n",
    "print(tokenizer.decode(tokenizer.encode(french_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fon_text = \"Àgɔ́! Ǹkɔ̀xɔ̀ wá, wɛ̀tɛ̀ ànù yì bó, ɖò lɛ̃̀, ɔ̃̀, ɛ̃́, ì, ú, ò, ɖɔ́, gbè, kpó, xù, ʋù, zã́. Sɛ́ wɛ́ ɖé ɖé, mí xó wɛ̀, é yà hùn dɔ̀ wɛ̃́. ʋɛ̀, ɖè, ɖɔ̀, mɛ̃̀, yì, gbɔ̀, sɔ̃̀, lɛ́ nɔ̀ ɖó lɛ́. Àɖó lɛ̀ wɛ̃̀ dɔ̀, ɔ̀kpà kpɛ́!\".lower()\n",
    "print(tokenizer.tokenize(fon_text))\n",
    "print(tokenizer.decode(tokenizer.encode(fon_text)))\n",
    "assert tokenizer.decode(tokenizer.encode(fon_text)) == fon_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fon_sentence = \"Wěma e ɖɔ xó dó Jezu Klísu, Davídi ví, Ablaxámu ví ɔ́ sín tɔ́gbó tɔgbo lɛ́ɛ wú ɔ́ ɖíe:\".lower()\n",
    "print(tokenizer.encode(fon_sentence))\n",
    "print(tokenizer.tokenize(fon_sentence))\n",
    "print(tokenizer.decode(tokenizer.encode(fon_sentence)))\n",
    "assert tokenizer.decode(tokenizer.encode(fon_sentence)) == fon_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_text = \"\\\"Pa kuti kasebanya naikila pali imwe, ali ne cipyu cickalamba, pa kwishibo kuti ali ne nshita inono fye.\\\" - Ukusokoloa 12:12.\".lower()\n",
    "print(tokenizer.tokenize(bem_text))\n",
    "print(tokenizer.decode(tokenizer.encode(bem_text)))\n",
    "assert tokenizer.decode(tokenizer.encode(bem_text)) == bem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make spec config instance\n",
    "spec_config = SpecConfig(\n",
    "    n_mels=80,\n",
    "    hop_length=256,\n",
    "    n_fft=1024,\n",
    "    sample_rate=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = STDataset(dataset_json=\"../corpora/train.json\",\n",
    "                           tokenizer=tokenizer,\n",
    "                            spec_config=spec_config,\n",
    "                            case_standardization=\"lower\")\n",
    "\n",
    "sample_loader = DataLoader(sample_dataset, batch_size=8, collate_fn=sample_dataset.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in sample_loader:\n",
    "    mels = batch[\"mel\"]\n",
    "    speech_lengths = batch[\"speech_lengths\"]\n",
    "    text_lengths = batch[\"text_lengths\"]\n",
    "    input_tokens = batch[\"input_tokens\"]\n",
    "    st_target_tokens = batch[\"st_target_tokens\"]\n",
    "    asr_target_tokens = batch[\"asr_target_tokens\"]\n",
    "\n",
    "    print(f\"mel shape: {mels.shape}\")\n",
    "    print(f\"text shape: {asr_target_tokens.shape}\")\n",
    "    print(f\"speech lengths: {speech_lengths}\")\n",
    "    print(f\"text lengths: {text_lengths}\")\n",
    "\n",
    "    for i in range(len(input_tokens)):\n",
    "        print(f\"input: {tokenizer.decode(input_tokens[i])}\")\n",
    "        print(f\"st target: {tokenizer.decode(st_target_tokens[i])}\")\n",
    "        print(f\"asr target: {tokenizer.decode(asr_target_tokens[i])}\")\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask_text = key_padding_mask(input_tokens, pad_idx=tokenizer.pad_token_id)\n",
    "print(f\"pad mask text shape: {pad_mask_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the masks\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(pad_mask_text, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask_speeech = key_padding_mask(mels.permute(0,2,1), speech_lengths)\n",
    "print(f\"pad mask speech shape: {pad_mask_speeech.shape}\")\n",
    "print(speech_lengths)\n",
    "# plot the masks\n",
    "plt.figure(figsize=(12, 24))\n",
    "plt.imshow(pad_mask_speeech, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask_text = causal_mask(input_tokens)\n",
    "print(f\"causal mask text shape: {causal_mask_text.shape}\")\n",
    "# plot the masks\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(causal_mask_text, cmap='gray', interpolation='nearest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
