# Model configuration
model:
  name: "small"
  d_model: 768
  n_heads: 12
  d_ff: 3072
  n_enc_layers: 12
  n_dec_layers: 12
  enc_attn_dropout: 0.1
  dec_attn_dropout: 0.1
  speech_embedding_type: "whisper"
  ff_type: "swiglu"
  pe_max_len: 16384
  vocab_size: 4096
  dtype: "float16"
  
# Model checkpoint (optional)
# model_checkpoint: "checkpoints/pretrained_model.pt"

# Tokenizer path
tokenizer: "alexgichamba/iwslt25_uncased_4096"

# Task configuration  
language_pairs:
  - source: bem
    target: eng
  - source: fon
    target: fra

# Task configuration  
task:
  source_language: "bem"
  target_language: "eng"

# Audio configuration
audio:
  n_mels: 80
  hop_length: 320
  n_fft: 1280
  sample_rate: 16000

# Text configuration
text:
  case_standardization: "lower"

# Training configuration
training:
  batch_duration: 1800
  bucket_length_multiplier: 1.6
  validation_batch_size: 64
  num_epochs: 5
  learning_rate: 2.0e-4
  weight_decay: 0.001
  device: "cuda"
  mixed_precision: true
  num_workers: 4
  scheduler: "CosineAnnealingWarmupRestarts"
  warmup_steps: 100
  cycle_length: 2 # epochs
  min_lr: 1.0e-6
  label_smoothing: 0.1
  ctcloss_weight: 0.3
  max_decoding_length: 256

# Data configuration
data:
  root: "corpora"
  audio_root: "corpora/audio"

# Logging configuration
logging:
  log_interval: 100
  wandb: true
  wandb_project: "e2e_st"
  wandb_run_name: "bem_eng_small_4096"

# Output configuration
output:
  dir: "output/bem_eng_small_4096"